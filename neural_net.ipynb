{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e326b479-da6d-44e9-a6a0-ca41776ad305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 How do you create a simple perceptron for basic binary classification?\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def perceptron(X, y, learning_rate=0.1, epochs=10):\n",
    "  \"\"\"\n",
    "  Trains a simple perceptron for binary classification.\n",
    "\n",
    "  Args:\n",
    "    X: Input features (numpy array).\n",
    "    y: Target labels (numpy array).\n",
    "    learning_rate: Learning rate for weight updates.\n",
    "    epochs: Number of training epochs.\n",
    "\n",
    "  Returns:\n",
    "    weights: Learned weights of the perceptron.\n",
    "    bias: Learned bias of the perceptron.\n",
    "  \"\"\"\n",
    "\n",
    "  weights = np.random.rand(X.shape[1])\n",
    "  bias = np.random.rand()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for i in range(len(X)):\n",
    "      prediction = np.dot(X[i], weights) + bias\n",
    "      error = y[i] - prediction\n",
    "\n",
    "      weights += learning_rate * error * X[i]\n",
    "      bias += learning_rate * error\n",
    "\n",
    "  return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed35686-bc22-4fd6-af78-04108edd99be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2 How can you build a neural network with one hidden layer using Keras?\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Generate random training data\n",
    "x_train = np.random.rand(1000, 2)\n",
    "y_train = np.random.rand(1000, 1)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, activation='sigmoid', input_dim=2))\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "x_test = np.random.rand(100, 2)\n",
    "y_test = np.random.rand(100, 1)\n",
    "loss, mae = model.evaluate(x_test, y_test)\n",
    "print(f\"Mean Absolute Error on test data: {mae}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d169c-baf9-4d07-a054-ff5b004d2df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3  How do you initialize weights using the Xavier (Glorot) initialization method in Keras?\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import GlorotUniform, GlorotNormal\n",
    "\n",
    "# Generate random training data\n",
    "x_train = np.random.rand(1000, 2)  # 1000 samples with 2 features\n",
    "y_train = np.random.rand(1000, 1)  # 1000 target values\n",
    "\n",
    "# Create the model using Glorot Uniform initialization\n",
    "model_uniform = Sequential()\n",
    "model_uniform.add(Dense(units=10, activation='relu', kernel_initializer=GlorotUniform(), input_dim=2))\n",
    "model_uniform.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model_uniform.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model_uniform.fit(x_train, y_train, epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "x_test = np.random.rand(100, 2)\n",
    "y_test = np.random.rand(100, 1)\n",
    "loss_uniform, mae_uniform = model_uniform.evaluate(x_test, y_test)\n",
    "print(f\"Mean Absolute Error on test data (Uniform): {mae_uniform}\")\n",
    "\n",
    "# Create the model using Glorot Normal initialization\n",
    "model_normal = Sequential()\n",
    "model_normal.add(Dense(units=10, activation='relu', kernel_initializer=GlorotNormal(), input_dim=2))\n",
    "model_normal.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model_normal.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model_normal.fit(x_train, y_train, epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss_normal, mae_normal = model_normal.evaluate(x_test, y_test)\n",
    "print(f\"Mean Absolute Error on test data (Normal): {mae_normal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafe546-72e9-48d4-8edb-8aa0dcd94328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4  How can you apply different activation functions in a neural network in Keras?\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding layers with different activation functions\n",
    "model.add(Dense(units=64, input_dim=10, activation='relu'))      # ReLU for first layer\n",
    "model.add(Dense(units=32, activation='sigmoid'))                # Sigmoid for second layer\n",
    "model.add(Dense(units=16, activation='tanh'))                   # Tanh for third layer\n",
    "model.add(Dense(units=3, activation='softmax'))                 # Softmax for output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (x_train and y_train should be defined)\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676debe0-b40c-4b2c-9fc1-98360e146cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5  How do you add dropout to a neural network model to prevent overfitting?\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Generate random training data\n",
    "x_train = np.random.rand(1000, 10)  # Example training data with 10 features\n",
    "y_train = np.random.randint(2, size=(1000, 1))  # Example binary labels\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer with Dropout\n",
    "model.add(Dense(units=64, activation='relu', input_dim=10))\n",
    "model.add(Dropout(0.5))  # Dropout layer with 50% dropout rate\n",
    "\n",
    "# Second hidden layer with Dropout\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout layer with 50% dropout rate\n",
    "\n",
    "# Output layer for binary classification\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=10)\n",
    "\n",
    "# Generate random test data\n",
    "x_test = np.random.rand(200, 10)  # Example test data with 10 features\n",
    "y_test = np.random.randint(2, size=(200, 1))  # Example binary labels\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134f86b-771c-4d52-b588-ee5149583a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6 How do you manually implement forward propagation in a simple neural network?\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize parameters\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    W1 = np.random.randn(hidden_size, input_size) * 0.01  # Weights for hidden layer\n",
    "    b1 = np.zeros((hidden_size, 1))  # Bias for hidden layer\n",
    "    W2 = np.random.randn(output_size, hidden_size) * 0.01  # Weights for output layer\n",
    "    b2 = np.zeros((output_size, 1))  # Bias for output layer\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Step 2: Define activation functions\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "# Step 3: Forward propagation function\n",
    "def forward_propagation(X):\n",
    "    W1, b1, W2, b2 = initialize_parameters(X.shape[0], 4, 1)  # Example sizes\n",
    "    Z1 = np.dot(W1, X) + b1  # Linear transformation for hidden layer\n",
    "    A1 = relu(Z1)            # Activation for hidden layer\n",
    "    Z2 = np.dot(W2, A1) + b2  # Linear transformation for output layer\n",
    "    A2 = sigmoid(Z2)         # Activation for output layer\n",
    "    return A2\n",
    "\n",
    "# Example input data (4 features)\n",
    "X = np.random.rand(4, 10)  # Batch of 10 samples\n",
    "\n",
    "# Perform forward propagation\n",
    "output = forward_propagation(X)\n",
    "print(\"Output of the neural network after forward propagation:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ff155-ecb0-4d0c-83f8-523ae732527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7  How do you add batch normalization to a neural network model in Keras?\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "\n",
    "# Generate random dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(1000, 10)  # 1000 samples, 10 features\n",
    "y = np.random.randint(2, size=(1000,))  # Binary labels\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer with Batch Normalization\n",
    "model.add(Dense(64, input_shape=(10,), activation='relu'))\n",
    "model.add(BatchNormalization())  # Batch normalization after activation\n",
    "\n",
    "# Second hidden layer with Batch Normalization\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())  # Batch normalization after activation\n",
    "\n",
    "# Output layer for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=20, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009111e-9824-4811-a836-fa29ef7aa722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8  How can you visualize the training process with accuracy and loss curves?\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate random dataset\n",
    "X = np.random.rand(1000, 10)  # 1000 samples, 10 features\n",
    "y = np.random.randint(2, size=(1000,))  # Binary labels\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model and store the history\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Plotting the accuracy and loss curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161b66c-d3b4-40f4-8c73-4d6ca9566a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9  How can you use gradient clipping in Keras to control the gradient size and prevent exploding gradients?\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile the model with gradient clipping by value\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(clipvalue=0.5),  # Clip gradients to be within [-0.5, 0.5]\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0714b4-130e-48b3-bc09-a3947e6002c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10 How can you create a custom loss function in Keras?\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define a custom loss function\n",
    "def custom_loss_function(y_true, y_pred):\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_difference)\n",
    "\n",
    "# Create a simple neural network model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dense(1))  # Output layer for regression\n",
    "\n",
    "# Compile the model with the custom loss function\n",
    "model.compile(optimizer='adam', loss=custom_loss_function)\n",
    "\n",
    "# Generate example training data\n",
    "X_train = np.random.rand(1000, 10)  # 1000 samples, 10 features\n",
    "y_train = np.random.rand(1000, 1)    # 1000 target values\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5923dc1-132c-4ec8-9d5e-8da256ad74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q11 How can you visualize the structure of a neural network model in Keras?\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import visualkeras\n",
    "\n",
    "# Generate random dataset\n",
    "X_train = np.random.rand(1000, 10)  # 1000 samples, 10 features\n",
    "y_train = np.random.randint(2, size=(1000,))  # Binary labels\n",
    "\n",
    "# Define a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Visualize the model architecture using plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "print(\"Model architecture saved as 'model_plot.png'.\")\n",
    "\n",
    "# Visualize the model architecture using Visualkeras\n",
    "visualkeras.layered_view(model).show()  # Display using your system viewer\n",
    "\n",
    "# Optionally, you can also save the Visualkeras output to a file\n",
    "# visualkeras.layered_view(model, to_file='visualkeras_model.png').show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667b310-45fa-40a2-a821-f41f53f26595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f7349-29ff-4997-b796-4683fce30388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31499b1d-018e-4fa9-9dc4-fefc8e437929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
